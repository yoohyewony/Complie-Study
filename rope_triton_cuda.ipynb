{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0eaf6b",
   "metadata": {},
   "source": [
    "# **Rotary Potion Embedding (RoPE) with Triton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7555712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention.\"\"\"\n",
    "import collections\n",
    "from contextlib import nullcontext\n",
    "import functools\n",
    "# from importlib.metadata import version\n",
    "import math\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from pkg_resources import packaging\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6efb4",
   "metadata": {},
   "source": [
    "### **RoPE in Pytoch from TransformerEngine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b64e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_engine.pytorch.attention import FusedRoPEFunc\n",
    "\n",
    "class RotaryPositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Rotary Position Embedding from https://arxiv.org/abs/2104.09864.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        rotary_percent: float = 1.0,\n",
    "        seq_len_interpolation_factor: Optional[int] = None,\n",
    "        pretrained_max_position_embeddings: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dim: int\n",
    "            rotary embedding dimension\n",
    "        rotary_percent: float\n",
    "            Percent of rotary dimension to use for rotary position embeddings.\n",
    "        seq_len_interpolation_factor: int\n",
    "            if not None, discrete positions will be interpolated by this factor via the trick in\n",
    "            https://arxiv.org/abs/2306.15595\n",
    "        pretrained_max_position_embeddings: int\n",
    "            pre-trained max_position_embeddings before position interpolation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rotary_percent < 1.0:\n",
    "            dim = int(dim * rotary_percent)\n",
    "        self.seq_len_interpolation_factor = seq_len_interpolation_factor\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (\n",
    "                torch.arange(0, dim, 2, dtype=torch.float32, device=torch.cuda.current_device())\n",
    "                / dim\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.pretrained_max_position_embeddings = pretrained_max_position_embeddings\n",
    "\n",
    "    def forward(self, max_seq_len: int, offset: int = 0):\n",
    "        \"\"\"\n",
    "        Create rotary position embedding frequencies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            sequence length of a sample\n",
    "        offset: int, default = 0\n",
    "            fixed offset for freqencies\n",
    "        \"\"\"\n",
    "        seq = (\n",
    "            torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "            + offset\n",
    "        )\n",
    "\n",
    "        if (self.pretrained_max_position_embeddings is not None\n",
    "            and self.seq_len_interpolation_factor is not None):\n",
    "            if (max_seq_len >\n",
    "                self.pretrained_max_position_embeddings * self.seq_len_interpolation_factor):\n",
    "                # dynamic linear scaling (length > position we have learned)\n",
    "                seq *= 1 / (max_seq_len / self.pretrained_max_position_embeddings)\n",
    "            else:\n",
    "                # fixed linear scaling\n",
    "                seq *= 1 / self.seq_len_interpolation_factor\n",
    "\n",
    "        freqs = torch.einsum('i , j -> i j', seq, self.inv_freq)\n",
    "        # first part even vector components, second part odd vector components,\n",
    "        #  2 * dim in dimension size\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        # emb [seq_length, .., dim]\n",
    "        return emb.reshape(emb.size(0), 1, 1, emb.size(1))\n",
    "\n",
    "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    change sign so the last dimension becomes [-odd, +even]\n",
    "    \"\"\"\n",
    "    x = x.view(x.shape[:-1] + torch.Size((2, x.shape[-1] // 2)))\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(\n",
    "    t: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    "    tensor_format: str = \"sbhd\",\n",
    "    fused: bool = False,\n",
    "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary positional embedding tensor to the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.Tensor\n",
    "        Input tensor of shape `[s, b, h, d]`, `[s, b, h, d]` or `[t, h, d]`, on which\n",
    "        rotary positional embedding will be applied.\n",
    "    freqs: torch.Tensor\n",
    "        Rotary positional embedding tensor of shape `[s2, 1, 1, d2]` and dtype 'float',\n",
    "        with `s2 >= s` and `d2 <= d`.\n",
    "    fused: bool, default = False\n",
    "        Whether to use a fused applying RoPE implementation.\n",
    "    tensor_format: {'sbhd', 'bshd', 'thd'}, default = 'sbhd'\n",
    "        is `bshd` if `t` is of shape `[bs, seq, ...]`, or `sbhd` if `t` is\n",
    "        of shape `[seq, bs, ...]`. 'thd' is only supported when `fused` is True.\n",
    "    cu_seqlens: torch.Tensor, default = None.\n",
    "        Cumulative sum of sequence lengths in a batch for `t`, with shape [b + 1] and\n",
    "        dtype torch.int32. Only valid when `tensor_format` is 'thd'.\n",
    "    \"\"\"\n",
    "    if fused:\n",
    "        assert (\n",
    "            tensor_format != \"thd\" or cu_seqlens is not None\n",
    "        ), \"cu_seqlens must not be None when tensor_format is 'thd'.\"\n",
    "        return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
    "\n",
    "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "        f\"when fused is False, got {tensor_format}.\"\n",
    "    )\n",
    "\n",
    "    max_seq_len = freqs.shape[0]\n",
    "    cur_seq_len = t.shape[1] if tensor_format == \"bshd\" else t.shape[0]\n",
    "\n",
    "    # Only apply the rotary embeddings up to the sequence length of the running\n",
    "    # input.\n",
    "    assert cur_seq_len <= max_seq_len, (\n",
    "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
    "    )\n",
    "    freqs = freqs[:cur_seq_len]\n",
    "    if tensor_format == \"bshd\":\n",
    "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
    "    # cos/sin first then dtype conversion for better precision\n",
    "    cos_ = torch.cos(freqs).to(t.dtype)\n",
    "    sin_ = torch.sin(freqs).to(t.dtype)\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t\n",
    "    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n",
    "\n",
    "    # first part is cosine component\n",
    "    # second part is sine component, need to change signs with _rotate_half method\n",
    "    t = (t * cos_) + (_rotate_half(t) * sin_)\n",
    "    return torch.cat((t, t_pass), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c026031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tol(dtype: torch.dtype) -> Dict:\n",
    "    if dtype == torch.bfloat16:\n",
    "        return dict(atol=1e-2, rtol=1e-2)\n",
    "    elif dtype == torch.float16:\n",
    "        return dict(atol=1e-3, rtol=1e-3)\n",
    "    return dict(atol=1e-5, rtol=1.3e-6)\n",
    "\n",
    "\n",
    "# Gradient is a broadcasted scalar\n",
    "def _overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    return output.sum() * 2\n",
    "\n",
    "# Gradient is a full tensor\n",
    "def _non_overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    t = torch.ones_like(output)\n",
    "    return torch.sum(output * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d443dfa",
   "metadata": {},
   "source": [
    "## **Triton**\n",
    "### **RoPE Forward**\n",
    "* RoPE에서 배치 간, 시퀀스 간, head 간 연산은 독립적이다. 하나의 임베딩 안에서 위치 변화(_rotate_half)를 하기 때문에 임베딩 내 연산만 메모리 접근 측면에서 독립적이지 않다.\n",
    "따라서 BLOCK_SIZE를 RoPE 연산을 적용하는 임베딩 차원의 크기로 설정한다. rotate_percent에 따라 RoPE 연산을 적용하는 임베딩의 갯수가 다르기 때문에 입력 텐서 $t$의 차원이 아닌 frequency embedding의 차원 수로 설정한다. 이 경우 한 블록 내에서 [1,1,1, freq_embedding_dim]의 메모리 접근이 가능하고 스레드마다 하나의 output을 계산한다.\n",
    "* 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & -\\sin m\\theta\\\\\n",
    "   \\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}\n",
    "$로 임베딩을 회전 변환하면 $(x_1\\cos m\\theta - x_2\\sin m\\theta, x_2\\cos m\\theta + x_1\\sin m\\theta)$의 결과가 나온다.\n",
    "여기서 sin, cos은 freq_emb(emb_ptr)을 순서대로 load하여 계산한다.\n",
    "_rotate_half를 하기 위해 입력 텐서(t_ptr)을 각각 t0, t1에 절반씩 load한다.\n",
    "out_ptr에 저장할 때 앞의 절반과 뒤의 절반의 연산이 다르기 때문에 각가 계산하여 다음과 같이 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac232b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def rope_fwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "    \n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos - t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos + t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6bb31",
   "metadata": {},
   "source": [
    "### **RoPE Backward**\n",
    "* RoPE forward와 마찬가지로 메모리 접근 측면에서 배치, 시퀀스, head 간 연산은 독립적이고 임베딩 간의 연산은 독립적이지 않기 때문에 BLOCK_SIZE를 frequency embedding의 차원 수로 설정한다.\n",
    "* backpropagation은 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & \\sin m\\theta\\\\\n",
    "   -\\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}$로 회전변환하면 된다. 따라서 rope_fwd()에서 결과값을 저장하는 부분에서 sin 앞의 부호만 바꿔주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def rope_bwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "\n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos + t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos - t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6ba38",
   "metadata": {},
   "source": [
    "### **RoPE Function**\n",
    "* rotary_percent에 따라 rotation을 하는 텐서 비율이 달라진다. rotation을 안하는 경우는 기존의 input 텐서 값을 그대로 보존하기 때문에 rope_fwd, rope_bwd 커널에 넣기 전에 output 텐서 값을 input 텐서 값으로 초기화한다. (복사하지 않아도 되는 값도 불러오기 때문에 시간이 더 소모될 수 있다고 예상한다.)\n",
    "* forward에서 받은 t와 freqs 텐서를 backward에서 다시 사용하기 위해 ctx.save_for_backward()로 저장한다.\n",
    "* BLOCK_SIZE는 freqs 텐서의 마지막 차원과 같은 크기로 지정한다.\n",
    "* grid는 input 텐서의 모든 차원 값을 곱한 값에서 BLOCK_SIZE로 나눈 값과 같다.\n",
    "* rope_fwd에서 출력된 텐서의 원소들의 메모리 주소가 연속적이지 않을 수 있다. 이를 해결하기 위해 contiguous()를 사용하여 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbeae841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tritonRoPE(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, t, freqs, tensor_format: str = \"sbhd\",\n",
    "                cu_seqlens: Union[torch.Tensor, None] = None,):\n",
    "        \n",
    "        # allocate output\n",
    "        output = t.clone().detach()\n",
    "        if tensor_format == \"sbhd\":\n",
    "            seq_length = t.shape[0]\n",
    "            batch_size = t.shape[1]\n",
    "            head_num = t.shape[2]\n",
    "            hidden_size = t.shape[3]\n",
    "            rot_dim = freqs.size()[-1]\n",
    "            BLOCK_SIZE=rot_dim\n",
    "            grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "            rope_fwd[grid](t, freqs, output, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "\n",
    "        ctx.save_for_backward(freqs, cu_seqlens)\n",
    "        ctx.tensor_format = tensor_format\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        freqs, cu_seqlens = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone().detach()\n",
    "        if ctx.tensor_format == \"sbhd\":\n",
    "            seq_length = grad_output.shape[0]\n",
    "            batch_size = grad_output.shape[1]\n",
    "            head_num = grad_output.shape[2]\n",
    "            hidden_size = grad_output.shape[3]\n",
    "            rot_dim = freqs.size()[-1]\n",
    "            grad_output = grad_output.contiguous()\n",
    "            grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "            rope_bwd[grid](grad_output, freqs, grad_input, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "\n",
    "        return grad_input, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b735574",
   "metadata": {},
   "source": [
    "### **Test RoPE**\n",
    "TransformerEngine에 있는 RoPE와 Triton kernel의 RoPE의 결과가 동일한지 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24a724a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## PASSED ##########\n"
     ]
    }
   ],
   "source": [
    "# @pytest.mark.parametrize(\"dtype\", [torch.float32, torch.bfloat16, torch.float16])\n",
    "# @pytest.mark.parametrize(\"seq_length\", [2048, 4096])\n",
    "# @pytest.mark.parametrize(\"hidden_size\", [128, 256])\n",
    "# @pytest.mark.parametrize(\"rotary_percent\", [0.5, 1.0])\n",
    "# @pytest.mark.parametrize(\"margin\", [0, 10])\n",
    "# @pytest.mark.parametrize(\"transpose\", [None, (0, 1), (2, 3)])\n",
    "# @pytest.mark.parametrize(\"tensor_format\", [\"sbhd\", \"bshd\"])\n",
    "# @pytest.mark.parametrize(\"loss_func\", [_overlapping_grad, _non_overlapping_grad])\n",
    "def test_fused_rope(\n",
    "    dtype: torch.dtype,\n",
    "    seq_length: int,\n",
    "    hidden_size: int,\n",
    "    rotary_percent: float,\n",
    "    margin: int,\n",
    "    transpose: Union[Tuple, None],\n",
    "    tensor_format: str,\n",
    "    loss_func: Callable,\n",
    ") -> None:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    batch_size, head_num = 2, 64\n",
    "    t = torch.rand(\n",
    "        (seq_length - margin, batch_size, head_num, hidden_size),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "    if transpose:\n",
    "        t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "\n",
    "    # unfused\n",
    "    output_unfused = apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    loss_unfused = loss_func(output_unfused)\n",
    "    loss_unfused.backward()\n",
    "    grad_unfused = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "    \n",
    "    #print(t.shape, emb.shape, output_unfused.shape)\n",
    "    \n",
    "    # Triton\n",
    "    output_triton = tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "    loss_triton = loss_func(output_triton)\n",
    "    loss_triton.backward()\n",
    "    grad_triton = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "\n",
    "    torch.testing.assert_close(output_triton, output_unfused)\n",
    "    torch.testing.assert_close(grad_triton, grad_unfused)\n",
    "\n",
    "#     torch.testing.assert_close(output_fused, output_unfused, **get_tol(dtype))\n",
    "#     torch.testing.assert_close(grad_fused, grad_unfused, **get_tol(dtype))\n",
    "#     assert output_fused.is_contiguous()\n",
    "    \n",
    "test_fused_rope(torch.float32, 2048, 128, 0.5, 10, None, \"sbhd\", _overlapping_grad)\n",
    "test_fused_rope(torch.float32, 2048, 128, 1.0, 0, None, \"sbhd\", _overlapping_grad)\n",
    "print(\"########## PASSED ##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453b404",
   "metadata": {},
   "source": [
    "### **Compare the Performance**\n",
    "시퀀스 길이(sequence_length)에 따른 Pytorch, Triton kernel, CUDA kernel로 작성한 RoPE의 성능을 비교했다. 배치 사이즈는 4, head 개수는 64, hidden_size는 128로 고정하고, sequence_length를 1280부터 4096까지 128씩 증가하여 성능을 확인했다. 모든 sequence length에 대해 Triton kernel이 기본 Pytorch 코드보다 forward와 backward 모두 2배 정도 빠른 것을 확인할 수 있다.  \n",
    "sequence_length를 x_vals로 주었지만, 배치 사이즈(batch_size), head의 개수(head_num), 임베딩 차원(hidden_size) 등으로 설정하여 이에 따른 성능도 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2fe1a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE performance:\n",
      "    seq_length      Triton       Torch        CUDA\n",
      "0       2048.0  315.456083  219.413274  668.734716\n",
      "1       2176.0  332.174128  221.405400  676.564852\n",
      "2       2304.0  330.804271  221.428781  675.628857\n",
      "3       2432.0  331.011901  221.405406  677.466802\n",
      "4       2560.0  331.016027  221.614819  676.326119\n",
      "5       2688.0  331.907872  221.405411  676.625406\n",
      "6       2816.0  331.294112  221.405398  672.477603\n",
      "7       2944.0  331.135310  221.503016  673.515626\n",
      "8       3072.0  331.403967  221.405403  670.337854\n",
      "9       3200.0  330.093807  221.236297  674.238653\n",
      "10      3328.0  332.086551  221.232927  674.559011\n",
      "11      3456.0  331.237728  221.183989  674.855850\n",
      "12      3584.0  331.228892  221.138579  673.150395\n",
      "13      3712.0  330.644398  220.993481  677.314340\n",
      "14      3840.0  330.673274  220.907871  672.854233\n",
      "15      3968.0  329.166561  221.116236  674.507330\n",
      "16      4096.0  331.303645  220.938904  667.033088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size, seq_length, head_num, hidden_size = 4, 4096, 64, 128    # batch size, sequence length, head_dim, emb_dim\n",
    "rotary_percent = 0.5\n",
    "mode = 'forward'\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seq_length'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(16, 33)],  # different possible values for `x_name`\n",
    "        #x_vals=[2048, 4096],\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch', 'cuda'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\", \"CUDA\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"RoPE performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={\"batch_size\": batch_size, \"head_num\": head_num, \"hidden_size\": hidden_size, \n",
    "              \"rotary_percent\": rotary_percent, \"tensor_format\": 'sbhd', \"mode\": mode},\n",
    "    ))\n",
    "def benchmark(batch_size, seq_length, head_num, hidden_size, provider, mode='forward', rotary_percent=1.0, tensor_format='sbhd'):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    t = torch.rand((batch_size, seq_length, head_num, hidden_size), dtype=torch.float32, device=device)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "#     if transpose:\n",
    "#         t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    if provider == 'triton':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "    if provider == 'cuda':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=True)\n",
    "        \n",
    "    # forward pass\n",
    "    if mode == 'forward':       \n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fwd(t, emb, tensor_format), quantiles=quantiles)\n",
    "    if mode == 'backward':\n",
    "        y = fn()\n",
    "        dy = torch.randn_like(y)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True), quantiles=quantiles, grad_to_none=[t])\n",
    "#     if provider == 'torch-compile':\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.compile(torch.square)(x), quantiles=quantiles)\n",
    "#     gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    gbps = lambda ms: 2 *t.numel() * t.element_size() / ms * 1e-6\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True, save_path='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
