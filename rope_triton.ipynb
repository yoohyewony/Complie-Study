{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e4ad55",
   "metadata": {},
   "source": [
    "# **Rotary Potion Embedding (RoPE) with Triton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124e06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention.\"\"\"\n",
    "import collections\n",
    "from contextlib import nullcontext\n",
    "import functools\n",
    "# from importlib.metadata import version\n",
    "import math\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from pkg_resources import packaging\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e20764",
   "metadata": {},
   "source": [
    "### **RoPE in Pytoch from TransformerEngine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c552131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Rotary Position Embedding from https://arxiv.org/abs/2104.09864.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        rotary_percent: float = 1.0,\n",
    "        seq_len_interpolation_factor: Optional[int] = None,\n",
    "        pretrained_max_position_embeddings: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dim: int\n",
    "            rotary embedding dimension\n",
    "        rotary_percent: float\n",
    "            Percent of rotary dimension to use for rotary position embeddings.\n",
    "        seq_len_interpolation_factor: int\n",
    "            if not None, discrete positions will be interpolated by this factor via the trick in\n",
    "            https://arxiv.org/abs/2306.15595\n",
    "        pretrained_max_position_embeddings: int\n",
    "            pre-trained max_position_embeddings before position interpolation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rotary_percent < 1.0:\n",
    "            dim = int(dim * rotary_percent)\n",
    "        self.seq_len_interpolation_factor = seq_len_interpolation_factor\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (\n",
    "                torch.arange(0, dim, 2, dtype=torch.float32, device=torch.cuda.current_device())\n",
    "                / dim\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.pretrained_max_position_embeddings = pretrained_max_position_embeddings\n",
    "\n",
    "    def forward(self, max_seq_len: int, offset: int = 0):\n",
    "        \"\"\"\n",
    "        Create rotary position embedding frequencies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            sequence length of a sample\n",
    "        offset: int, default = 0\n",
    "            fixed offset for freqencies\n",
    "        \"\"\"\n",
    "        seq = (\n",
    "            torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "            + offset\n",
    "        )\n",
    "\n",
    "        if (self.pretrained_max_position_embeddings is not None\n",
    "            and self.seq_len_interpolation_factor is not None):\n",
    "            if (max_seq_len >\n",
    "                self.pretrained_max_position_embeddings * self.seq_len_interpolation_factor):\n",
    "                # dynamic linear scaling (length > position we have learned)\n",
    "                seq *= 1 / (max_seq_len / self.pretrained_max_position_embeddings)\n",
    "            else:\n",
    "                # fixed linear scaling\n",
    "                seq *= 1 / self.seq_len_interpolation_factor\n",
    "\n",
    "        freqs = torch.einsum('i , j -> i j', seq, self.inv_freq)\n",
    "        # first part even vector components, second part odd vector components,\n",
    "        #  2 * dim in dimension size\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        # emb [seq_length, .., dim]\n",
    "        return emb.reshape(emb.size(0), 1, 1, emb.size(1))\n",
    "\n",
    "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    change sign so the last dimension becomes [-odd, +even]\n",
    "    \"\"\"\n",
    "    x = x.view(x.shape[:-1] + torch.Size((2, x.shape[-1] // 2)))\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(\n",
    "    t: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    "    tensor_format: str = \"sbhd\",\n",
    "    fused: bool = False,\n",
    "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary positional embedding tensor to the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.Tensor\n",
    "        Input tensor of shape `[s, b, h, d]`, `[s, b, h, d]` or `[t, h, d]`, on which\n",
    "        rotary positional embedding will be applied.\n",
    "    freqs: torch.Tensor\n",
    "        Rotary positional embedding tensor of shape `[s2, 1, 1, d2]` and dtype 'float',\n",
    "        with `s2 >= s` and `d2 <= d`.\n",
    "    fused: bool, default = False\n",
    "        Whether to use a fused applying RoPE implementation.\n",
    "    tensor_format: {'sbhd', 'bshd', 'thd'}, default = 'sbhd'\n",
    "        is `bshd` if `t` is of shape `[bs, seq, ...]`, or `sbhd` if `t` is\n",
    "        of shape `[seq, bs, ...]`. 'thd' is only supported when `fused` is True.\n",
    "    cu_seqlens: torch.Tensor, default = None.\n",
    "        Cumulative sum of sequence lengths in a batch for `t`, with shape [b + 1] and\n",
    "        dtype torch.int32. Only valid when `tensor_format` is 'thd'.\n",
    "    \"\"\"\n",
    "#     if fused:\n",
    "#         assert (\n",
    "#             tensor_format != \"thd\" or cu_seqlens is not None\n",
    "#         ), \"cu_seqlens must not be None when tensor_format is 'thd'.\"\n",
    "#         return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
    "\n",
    "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "        f\"when fused is False, got {tensor_format}.\"\n",
    "    )\n",
    "\n",
    "    max_seq_len = freqs.shape[0]\n",
    "    cur_seq_len = t.shape[1] if tensor_format == \"bshd\" else t.shape[0]\n",
    "\n",
    "    # Only apply the rotary embeddings up to the sequence length of the running\n",
    "    # input.\n",
    "    assert cur_seq_len <= max_seq_len, (\n",
    "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
    "    )\n",
    "    freqs = freqs[:cur_seq_len]\n",
    "    if tensor_format == \"bshd\":\n",
    "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
    "    # cos/sin first then dtype conversion for better precision\n",
    "    cos_ = torch.cos(freqs).to(t.dtype)\n",
    "    sin_ = torch.sin(freqs).to(t.dtype)\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t\n",
    "    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n",
    "\n",
    "    # first part is cosine component\n",
    "    # second part is sine component, need to change signs with _rotate_half method\n",
    "    t = (t * cos_) + (_rotate_half(t) * sin_)\n",
    "    return torch.cat((t, t_pass), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52989b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tol(dtype: torch.dtype) -> Dict:\n",
    "    if dtype == torch.bfloat16:\n",
    "        return dict(atol=1e-2, rtol=1e-2)\n",
    "    elif dtype == torch.float16:\n",
    "        return dict(atol=1e-3, rtol=1e-3)\n",
    "    return dict(atol=1e-5, rtol=1.3e-6)\n",
    "\n",
    "\n",
    "# Gradient is a broadcasted scalar\n",
    "def _overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    return output.sum() * 2\n",
    "\n",
    "# Gradient is a full tensor\n",
    "def _non_overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    t = torch.ones_like(output)\n",
    "    return torch.sum(output * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb4abb",
   "metadata": {},
   "source": [
    "## **Triton**\n",
    "### **RoPE Forward**\n",
    "* RoPE에서 배치 간, 시퀀스 간, head 간 연산은 독립적이다. 하나의 임베딩 안에서 위치 변화(_rotate_half)를 하기 때문에 임베딩 내 연산만 메모리 접근 측면에서 독립적이지 않다.\n",
    "따라서 BLOCK_SIZE를 RoPE 연산을 적용하는 임베딩 차원의 크기로 설정한다. rotate_percent에 따라 RoPE 연산을 적용하는 임베딩의 갯수가 다르기 때문에 입력 텐서 $t$의 차원이 아닌 frequency embedding의 차원 수로 설정한다. 이 경우 한 블록 내에서 [1,1,1, freq_embedding_dim]의 메모리 접근이 가능하고 스레드마다 하나의 output을 계산한다.\n",
    "* 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & -\\sin m\\theta\\\\\n",
    "   \\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}\n",
    "$로 임베딩을 회전 변환하면 $(x_1\\cos m\\theta - x_2\\sin m\\theta, x_2\\cos m\\theta + x_1\\sin m\\theta)$의 결과가 나온다.\n",
    "여기서 sin, cos은 freq_emb(emb_ptr)을 순서대로 load하여 계산한다.\n",
    "_rotate_half를 하기 위해 입력 텐서(t_ptr)을 각각 t0, t1에 절반씩 load한다.\n",
    "out_ptr에 저장할 때 앞의 절반과 뒤의 절반의 연산이 다르기 때문에 각가 계산하여 다음과 같이 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f00a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def rope_fwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "    \n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos - t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos + t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456acdd0",
   "metadata": {},
   "source": [
    "### **RoPE Backward**\n",
    "* RoPE forward와 마찬가지로 메모리 접근 측면에서 배치, 시퀀스, head 간 연산은 독립적이고 임베딩 간의 연산은 독립적이지 않기 때문에 BLOCK_SIZE를 frequency embedding의 차원 수로 설정한다.\n",
    "* backpropagation은 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & \\sin m\\theta\\\\\n",
    "   -\\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}$로 회전변환하면 된다. 따라서 rope_fwd()에서 결과값을 저장하는 부분에서 sin 앞의 부호만 바꿔주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11585c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def rope_bwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "\n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos + t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos - t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80dee7",
   "metadata": {},
   "source": [
    "### **RoPE Function**\n",
    "* rotary_percent에 따라 rotation을 하는 텐서 비율이 달라진다. rotation을 안하는 경우는 기존의 input 텐서 값을 그대로 보존하기 때문에 rope_fwd, rope_bwd 커널에 넣기 전에 output 텐서 값을 input 텐서 값으로 초기화한다. (복사하지 않아도 되는 값도 불러오기 때문에 시간이 더 소모될 수 있다고 예상한다.)\n",
    "* forward에서 받은 t와 freqs 텐서를 backward에서 다시 사용하기 위해 ctx.save_for_backward()로 저장한다.\n",
    "* BLOCK_SIZE는 freqs 텐서의 마지막 차원과 같은 크기로 지정한다.\n",
    "* grid는 input 텐서의 모든 차원 값을 곱한 값에서 BLOCK_SIZE로 나눈 값과 같다.\n",
    "* rope_fwd에서 출력된 텐서의 원소들의 메모리 주소가 연속적이지 않을 수 있다. 이를 해결하기 위해 contiguous()를 사용하여 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6254e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tritonRoPE(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, t, freqs, tensor_format: str = \"sbhd\",\n",
    "                cu_seqlens: Union[torch.Tensor, None] = None,):\n",
    "        \n",
    "        # allocate output\n",
    "        output = t.clone().detach()\n",
    "        if tensor_format == \"sbhd\":\n",
    "            seq_length = t.shape[0]\n",
    "            batch_size = t.shape[1]\n",
    "            head_num = t.shape[2]\n",
    "            hidden_size = t.shape[3]\n",
    "            rot_dim = freqs.size()[-1]\n",
    "            BLOCK_SIZE=rot_dim\n",
    "            grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "            rope_fwd[grid](t, freqs, output, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "\n",
    "        ctx.save_for_backward(freqs, cu_seqlens)\n",
    "        ctx.tensor_format = tensor_format\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        freqs, cu_seqlens = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone().detach()\n",
    "        if ctx.tensor_format == \"sbhd\":\n",
    "            seq_length = grad_output.shape[0]\n",
    "            batch_size = grad_output.shape[1]\n",
    "            head_num = grad_output.shape[2]\n",
    "            hidden_size = grad_output.shape[3]\n",
    "            rot_dim = freqs.size()[-1]\n",
    "            grad_output = grad_output.contiguous()\n",
    "            grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "            rope_bwd[grid](grad_output, freqs, grad_input, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "\n",
    "        return grad_input, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e74ca",
   "metadata": {},
   "source": [
    "### **Test RoPE**\n",
    "TransformerEngine에 있는 RoPE와 Triton kernel의 RoPE의 결과가 동일한지 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d85afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## PASSED ##########\n"
     ]
    }
   ],
   "source": [
    "# @pytest.mark.parametrize(\"dtype\", [torch.float32, torch.bfloat16, torch.float16])\n",
    "# @pytest.mark.parametrize(\"seq_length\", [2048, 4096])\n",
    "# @pytest.mark.parametrize(\"hidden_size\", [128, 256])\n",
    "# @pytest.mark.parametrize(\"rotary_percent\", [0.5, 1.0])\n",
    "# @pytest.mark.parametrize(\"margin\", [0, 10])\n",
    "# @pytest.mark.parametrize(\"transpose\", [None, (0, 1), (2, 3)])\n",
    "# @pytest.mark.parametrize(\"tensor_format\", [\"sbhd\", \"bshd\"])\n",
    "# @pytest.mark.parametrize(\"loss_func\", [_overlapping_grad, _non_overlapping_grad])\n",
    "def test_fused_rope(\n",
    "    dtype: torch.dtype,\n",
    "    seq_length: int,\n",
    "    hidden_size: int,\n",
    "    rotary_percent: float,\n",
    "    margin: int,\n",
    "    transpose: Union[Tuple, None],\n",
    "    tensor_format: str,\n",
    "    loss_func: Callable,\n",
    ") -> None:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    batch_size, head_num = 2, 64\n",
    "    t = torch.rand(\n",
    "        (seq_length - margin, batch_size, head_num, hidden_size),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "    if transpose:\n",
    "        t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "\n",
    "    # unfused\n",
    "    output_unfused = apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    loss_unfused = loss_func(output_unfused)\n",
    "    loss_unfused.backward()\n",
    "    grad_unfused = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "    \n",
    "    #print(t.shape, emb.shape, output_unfused.shape)\n",
    "    \n",
    "    # Triton\n",
    "    output_triton = tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "    loss_triton = loss_func(output_triton)\n",
    "    loss_triton.backward()\n",
    "    grad_triton = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "\n",
    "    torch.testing.assert_close(output_triton, output_unfused)\n",
    "    torch.testing.assert_close(grad_triton, grad_unfused)\n",
    "\n",
    "#     torch.testing.assert_close(output_fused, output_unfused, **get_tol(dtype))\n",
    "#     torch.testing.assert_close(grad_fused, grad_unfused, **get_tol(dtype))\n",
    "#     assert output_fused.is_contiguous()\n",
    "    \n",
    "test_fused_rope(torch.float32, 2048, 128, 0.5, 10, None, \"sbhd\", _overlapping_grad)\n",
    "test_fused_rope(torch.float32, 2048, 128, 1.0, 0, None, \"sbhd\", _overlapping_grad)\n",
    "print(\"########## PASSED ##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e6916",
   "metadata": {},
   "source": [
    "### **Compare the Performance**\n",
    "시퀀스 길이(sequence_length)에 따른 Pytorch, Triton kernel, CUDA kernel로 작성한 RoPE의 성능을 비교했다. 배치 사이즈는 4, head 개수는 64, hidden_size는 128로 고정하고, sequence_length를 1280부터 4096까지 128씩 증가하여 성능을 확인했다. 모든 sequence length에 대해 Triton kernel이 기본 Pytorch 코드보다 forward와 backward 모두 2배 정도 빠른 것을 확인할 수 있다.  \n",
    "sequence_length를 x_vals로 주었지만, 배치 사이즈(batch_size), head의 개수(head_num), 임베딩 차원(hidden_size) 등으로 설정하여 이에 따른 성능도 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf86f5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE performance:\n",
      "    seq_length      Triton       Torch\n",
      "0       1280.0  193.523128  138.742742\n",
      "1       1408.0  229.905040  139.635800\n",
      "2       1536.0  231.883231  139.670453\n",
      "3       1664.0  231.993730  139.053539\n",
      "4       1792.0  232.033509  138.942725\n",
      "5       1920.0  231.271234  138.948645\n",
      "6       2048.0  231.320547  138.306059\n",
      "7       2176.0  231.385087  138.446300\n",
      "8       2304.0  231.391432  137.962450\n",
      "9       2432.0  231.401135  137.852156\n",
      "10      2560.0  231.475009  137.608411\n",
      "11      2688.0  231.545534  137.401025\n",
      "12      2816.0  231.506247  136.949951\n",
      "13      2944.0  230.576350  137.073827\n",
      "14      3072.0  230.504812  136.816886\n",
      "15      3200.0  230.532254  136.796948\n",
      "16      3328.0  230.606369  136.382379\n",
      "17      3456.0  230.555735  136.358737\n",
      "18      3584.0  230.615550  136.253908\n",
      "19      3712.0  229.775388  136.346619\n",
      "20      3840.0  230.650595  136.332442\n",
      "21      3968.0  230.623042  135.898587\n",
      "22      4096.0  229.749351  135.793477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size, seq_length, head_num, hidden_size = 4, 4096, 64, 128    # batch size, sequence length, head_dim, emb_dim\n",
    "rotary_percent = 0.5\n",
    "mode = 'forward'\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seq_length'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(10, 33)],  # different possible values for `x_name`\n",
    "        #x_vals=[2048, 4096],\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')], #('orage', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"RoPE performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={\"batch_size\": batch_size, \"head_num\": head_num, \"hidden_size\": hidden_size, \n",
    "              \"rotary_percent\": rotary_percent, \"tensor_format\": 'sbhd', \"mode\": mode},\n",
    "    ))\n",
    "def benchmark(batch_size, seq_length, head_num, hidden_size, provider, mode='forward', rotary_percent=1.0, tensor_format='sbhd'):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    t = torch.rand((batch_size, seq_length, head_num, hidden_size), dtype=torch.float32, device=device)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "        tensor_format == \"sbhd\"\n",
    "#     if transpose:\n",
    "#         t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    if provider == 'triton':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "#     if provider == 'cuda':\n",
    "#         def fwd(t, emb, tensor_format):\n",
    "#             return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=True)\n",
    "        \n",
    "    # forward pass\n",
    "    if mode == 'forward':       \n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fwd(t, emb, tensor_format), quantiles=quantiles)\n",
    "    if mode == 'backward':\n",
    "        y = fwd(t, emb, tensor_format)\n",
    "        dy = torch.randn_like(y)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True), quantiles=quantiles, grad_to_none=[t])\n",
    "\n",
    "    gbps = lambda ms: 2*t.numel() * t.element_size() / ms * 1e-6\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True, save_path='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
