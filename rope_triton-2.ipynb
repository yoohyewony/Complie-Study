{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac5bac0",
   "metadata": {},
   "source": [
    "# **Rotary Potion Embedding (RoPE) with Triton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317aa02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention.\"\"\"\n",
    "import collections\n",
    "from contextlib import nullcontext\n",
    "import functools\n",
    "# from importlib.metadata import version\n",
    "import math\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from pkg_resources import packaging\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7b290",
   "metadata": {},
   "source": [
    "### **RoPE in Pytoch from TransformerEngine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb9f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Rotary Position Embedding from https://arxiv.org/abs/2104.09864.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        rotary_percent: float = 1.0,\n",
    "        seq_len_interpolation_factor: Optional[int] = None,\n",
    "        pretrained_max_position_embeddings: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dim: int\n",
    "            rotary embedding dimension\n",
    "        rotary_percent: float\n",
    "            Percent of rotary dimension to use for rotary position embeddings.\n",
    "        seq_len_interpolation_factor: int\n",
    "            if not None, discrete positions will be interpolated by this factor via the trick in\n",
    "            https://arxiv.org/abs/2306.15595\n",
    "        pretrained_max_position_embeddings: int\n",
    "            pre-trained max_position_embeddings before position interpolation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rotary_percent < 1.0:\n",
    "            dim = int(dim * rotary_percent)\n",
    "        self.seq_len_interpolation_factor = seq_len_interpolation_factor\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (\n",
    "                torch.arange(0, dim, 2, dtype=torch.float32, device=torch.cuda.current_device())\n",
    "                / dim\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.pretrained_max_position_embeddings = pretrained_max_position_embeddings\n",
    "\n",
    "    def forward(self, max_seq_len: int, offset: int = 0):\n",
    "        \"\"\"\n",
    "        Create rotary position embedding frequencies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            sequence length of a sample\n",
    "        offset: int, default = 0\n",
    "            fixed offset for freqencies\n",
    "        \"\"\"\n",
    "        seq = (\n",
    "            torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "            + offset\n",
    "        )\n",
    "\n",
    "        if (self.pretrained_max_position_embeddings is not None\n",
    "            and self.seq_len_interpolation_factor is not None):\n",
    "            if (max_seq_len >\n",
    "                self.pretrained_max_position_embeddings * self.seq_len_interpolation_factor):\n",
    "                # dynamic linear scaling (length > position we have learned)\n",
    "                seq *= 1 / (max_seq_len / self.pretrained_max_position_embeddings)\n",
    "            else:\n",
    "                # fixed linear scaling\n",
    "                seq *= 1 / self.seq_len_interpolation_factor\n",
    "\n",
    "        freqs = torch.einsum('i , j -> i j', seq, self.inv_freq)\n",
    "        # first part even vector components, second part odd vector components,\n",
    "        #  2 * dim in dimension size\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        # emb [seq_length, .., dim]\n",
    "        return emb.reshape(emb.size(0), 1, 1, emb.size(1))\n",
    "\n",
    "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    change sign so the last dimension becomes [-odd, +even]\n",
    "    \"\"\"\n",
    "    x = x.view(x.shape[:-1] + torch.Size((2, x.shape[-1] // 2)))\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(\n",
    "    t: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    "    tensor_format: str = \"sbhd\",\n",
    "    fused: bool = False,\n",
    "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary positional embedding tensor to the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.Tensor\n",
    "        Input tensor of shape `[s, b, h, d]`, `[s, b, h, d]` or `[t, h, d]`, on which\n",
    "        rotary positional embedding will be applied.\n",
    "    freqs: torch.Tensor\n",
    "        Rotary positional embedding tensor of shape `[s2, 1, 1, d2]` and dtype 'float',\n",
    "        with `s2 >= s` and `d2 <= d`.\n",
    "    fused: bool, default = False\n",
    "        Whether to use a fused applying RoPE implementation.\n",
    "    tensor_format: {'sbhd', 'bshd', 'thd'}, default = 'sbhd'\n",
    "        is `bshd` if `t` is of shape `[bs, seq, ...]`, or `sbhd` if `t` is\n",
    "        of shape `[seq, bs, ...]`. 'thd' is only supported when `fused` is True.\n",
    "    cu_seqlens: torch.Tensor, default = None.\n",
    "        Cumulative sum of sequence lengths in a batch for `t`, with shape [b + 1] and\n",
    "        dtype torch.int32. Only valid when `tensor_format` is 'thd'.\n",
    "    \"\"\"\n",
    "#     if fused:\n",
    "#         assert (\n",
    "#             tensor_format != \"thd\" or cu_seqlens is not None\n",
    "#         ), \"cu_seqlens must not be None when tensor_format is 'thd'.\"\n",
    "#         return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
    "\n",
    "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "        f\"when fused is False, got {tensor_format}.\"\n",
    "    )\n",
    "\n",
    "    max_seq_len = freqs.shape[0]\n",
    "    cur_seq_len = t.shape[1] if tensor_format == \"bshd\" else t.shape[0]\n",
    "\n",
    "    # Only apply the rotary embeddings up to the sequence length of the running\n",
    "    # input.\n",
    "    assert cur_seq_len <= max_seq_len, (\n",
    "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
    "    )\n",
    "    freqs = freqs[:cur_seq_len]\n",
    "    if tensor_format == \"bshd\":\n",
    "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
    "    # cos/sin first then dtype conversion for better precision\n",
    "    cos_ = torch.cos(freqs).to(t.dtype)\n",
    "    sin_ = torch.sin(freqs).to(t.dtype)\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t\n",
    "    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n",
    "\n",
    "    # first part is cosine component\n",
    "    # second part is sine component, need to change signs with _rotate_half method\n",
    "    t = (t * cos_) + (_rotate_half(t) * sin_)\n",
    "    return torch.cat((t, t_pass), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a56a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tol(dtype: torch.dtype) -> Dict:\n",
    "    if dtype == torch.bfloat16:\n",
    "        return dict(atol=1e-2, rtol=1e-2)\n",
    "    elif dtype == torch.float16:\n",
    "        return dict(atol=1e-3, rtol=1e-3)\n",
    "    return dict(atol=1e-5, rtol=1.3e-6)\n",
    "\n",
    "\n",
    "# Gradient is a broadcasted scalar\n",
    "def _overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    return output.sum() * 2\n",
    "\n",
    "# Gradient is a full tensor\n",
    "def _non_overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    t = torch.ones_like(output)\n",
    "    return torch.sum(output * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536757f",
   "metadata": {},
   "source": [
    "## **Triton**\n",
    "### **RoPE Forward**\n",
    "* RoPE에서 배치 간, 시퀀스 간, head 간 연산은 독립적이다. 하나의 임베딩 안에서 위치 변화(_rotate_half)를 하기 때문에 임베딩 내 연산만 메모리 접근 측면에서 독립적이지 않다.\n",
    "따라서 BLOCK_SIZE를 RoPE 연산을 적용하는 임베딩 차원의 크기로 설정한다. rotate_percent에 따라 RoPE 연산을 적용하는 임베딩의 갯수가 다르기 때문에 입력 텐서 $t$의 차원이 아닌 frequency embedding의 차원 수로 설정한다. 이 경우 한 블록 내에서 [1,1,1, freq_embedding_dim]의 메모리 접근이 가능하고 스레드마다 하나의 output을 계산한다.\n",
    "* 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & -\\sin m\\theta\\\\\n",
    "   \\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}\n",
    "$로 임베딩을 회전 변환하면 $(x_1\\cos m\\theta - x_2\\sin m\\theta, x_2\\cos m\\theta + x_1\\sin m\\theta)$의 결과가 나온다.\n",
    "여기서 sin, cos은 freq_emb(emb_ptr)을 순서대로 load하여 계산한다.\n",
    "_rotate_half를 하기 위해 입력 텐서(t_ptr)을 각각 t0, t1에 절반씩 load한다.\n",
    "out_ptr에 저장할 때 앞의 절반과 뒤의 절반의 연산이 다르기 때문에 각가 계산하여 다음과 같이 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f623f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def rope_fwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "    \n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos - t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos + t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205acf23",
   "metadata": {},
   "source": [
    "### **RoPE Backward**\n",
    "* RoPE forward와 마찬가지로 메모리 접근 측면에서 배치, 시퀀스, head 간 연산은 독립적이고 임베딩 간의 연산은 독립적이지 않기 때문에 BLOCK_SIZE를 frequency embedding의 차원 수로 설정한다.\n",
    "* backpropagation은 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & \\sin m\\theta\\\\\n",
    "   -\\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}$로 회전변환하면 된다. 따라서 rope_fwd()에서 결과값을 저장하는 부분에서 sin 앞의 부호만 바꿔주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4384a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def rope_bwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "\n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos + t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos - t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737a55a",
   "metadata": {},
   "source": [
    "### **RoPE Function**\n",
    "* rotary_percent에 따라 rotation을 하는 텐서 비율이 달라진다. rotation을 안하는 경우는 기존의 input 텐서 값을 그대로 보존하기 때문에 rope_fwd, rope_bwd 커널에 넣기 전에 output 텐서 값을 input 텐서 값으로 초기화한다. (복사하지 않아도 되는 값도 불러오기 때문에 시간이 더 소모될 수 있다고 예상한다.)\n",
    "* 가속화 하기 위해서는 BLOCK_SIZE를 1024에 가깝게 하는 게 좋다. 따라서 head_num도 같이 flatten하여 넣어주면 빨라질 것으로 예상하지만 시간 관계상 패스했다.\n",
    "* forward에서 받은 t와 freqs 텐서를 backward에서 다시 사용하기 위해 ctx.save_for_backward()로 저장한다.\n",
    "* BLOCK_SIZE는 freqs 텐서의 마지막 차원과 같은 크기로 지정한다.\n",
    "* grid는 input 텐서의 모든 차원 값을 곱한 값에서 BLOCK_SIZE로 나눈 값과 같다.\n",
    "* rope_fwd에서 출력된 텐서의 원소들의 메모리 주소가 연속적이지 않을 수 있다. 이를 해결하기 위해 contiguous()를 사용하여 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22e3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tritonRoPE(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, t, freqs, tensor_format: str = \"sbhd\",\n",
    "                cu_seqlens: Union[torch.Tensor, None] = None,):\n",
    "        \n",
    "        assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "            \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "            f\"when fused is False, got {tensor_format}.\"\n",
    "        )\n",
    "        # allocate output\n",
    "        if tensor_format == \"bshd\":\n",
    "            t = t.transpose(0, 1).contiguous()\n",
    "            freqs = freqs.transpose(0, 1).contiguous()\n",
    "            \n",
    "        output = t.clone().detach()\n",
    "        seq_length = t.shape[0]\n",
    "        batch_size = t.shape[1]\n",
    "        head_num = t.shape[2]\n",
    "        hidden_size = t.shape[3]\n",
    "        rot_dim = freqs.size()[-1]\n",
    "        grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "        rope_fwd[grid](t, freqs, output, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "        \n",
    "        if tensor_format == \"bshd\":\n",
    "            output = output.transpose(0, 1)\n",
    "            \n",
    "        ctx.save_for_backward(freqs, cu_seqlens)\n",
    "        ctx.tensor_format = tensor_format\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        freqs, cu_seqlens = ctx.saved_tensors\n",
    "        if ctx.tensor_format == \"bshd\":\n",
    "            grad_output = grad_output.transpose(0, 1).contiguous()\n",
    "            \n",
    "        grad_input = grad_output.clone().detach()    \n",
    "        seq_length = grad_output.shape[0]\n",
    "        batch_size = grad_output.shape[1]\n",
    "        head_num = grad_output.shape[2]\n",
    "        hidden_size = grad_output.shape[3]\n",
    "        rot_dim = freqs.size()[-1]\n",
    "        grad_output = grad_output.contiguous()\n",
    "        grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "        rope_bwd[grid](grad_output, freqs, grad_input, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "        \n",
    "        if ctx.tensor_format == \"bshd\":\n",
    "            grad_input = grad_input.transpose(0, 1)\n",
    "        return grad_input, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85857478",
   "metadata": {},
   "source": [
    "### **Test RoPE**\n",
    "TransformerEngine에 있는 RoPE와 Triton kernel의 RoPE의 결과가 동일한지 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9730747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## PASSED ##########\n"
     ]
    }
   ],
   "source": [
    "# @pytest.mark.parametrize(\"dtype\", [torch.float32, torch.bfloat16, torch.float16])\n",
    "# @pytest.mark.parametrize(\"seq_length\", [2048, 4096])\n",
    "# @pytest.mark.parametrize(\"hidden_size\", [128, 256])\n",
    "# @pytest.mark.parametrize(\"rotary_percent\", [0.5, 1.0])\n",
    "# @pytest.mark.parametrize(\"margin\", [0, 10])\n",
    "# @pytest.mark.parametrize(\"transpose\", [None, (0, 1), (2, 3)])\n",
    "# @pytest.mark.parametrize(\"tensor_format\", [\"sbhd\", \"bshd\"])\n",
    "# @pytest.mark.parametrize(\"loss_func\", [_overlapping_grad, _non_overlapping_grad])\n",
    "def test_fused_rope(\n",
    "    dtype: torch.dtype,\n",
    "    seq_length: int,\n",
    "    hidden_size: int,\n",
    "    rotary_percent: float,\n",
    "    margin: int,\n",
    "    transpose: Union[Tuple, None],\n",
    "    tensor_format: str,\n",
    "    loss_func: Callable,\n",
    ") -> None:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    batch_size, head_num = 2, 64\n",
    "    t = torch.rand(\n",
    "        (seq_length - margin, batch_size, head_num, hidden_size),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "    if transpose:\n",
    "        t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "\n",
    "    # unfused\n",
    "    output_unfused = apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    loss_unfused = loss_func(output_unfused)\n",
    "    loss_unfused.backward()\n",
    "    grad_unfused = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "    \n",
    "    #print(t.shape, emb.shape, output_unfused.shape)\n",
    "    \n",
    "    # Triton\n",
    "    output_triton = tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "    loss_triton = loss_func(output_triton)\n",
    "    loss_triton.backward()\n",
    "    grad_triton = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "\n",
    "    torch.testing.assert_close(output_triton, output_unfused)\n",
    "    torch.testing.assert_close(grad_triton, grad_unfused)\n",
    "\n",
    "    \n",
    "test_fused_rope(torch.float32, 2048, 128, 0.5, 10, None, \"sbhd\", _overlapping_grad)\n",
    "test_fused_rope(torch.float32, 2048, 128, 1.0, 0, None, \"sbhd\", _overlapping_grad)\n",
    "print(\"########## PASSED ##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3a279",
   "metadata": {},
   "source": [
    "### **Compare the Performance**\n",
    "시퀀스 길이(sequence_length)에 따른 Pytorch, Triton kernel, CUDA kernel로 작성한 RoPE의 성능을 비교했다. 배치 사이즈는 4, head 개수는 64, hidden_size는 128로 고정하고, sequence_length를 1280부터 4096까지 128씩 증가하여 성능을 확인했다. 모든 sequence length에 대해 Triton kernel이 기본 Pytorch 코드보다 forward와 backward 모두 2배 정도 빠른 것을 확인할 수 있다.  \n",
    "sequence_length를 x_vals로 주었지만, 배치 사이즈(batch_size), head의 개수(head_num), 임베딩 차원(hidden_size) 등으로 설정하여 이에 따른 성능도 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87359159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoPE performance:\n",
      "    seq_length      Triton       Torch\n",
      "0       1280.0  196.057819  140.654052\n",
      "1       1408.0  232.584613  139.926323\n",
      "2       1536.0  232.818556  139.607701\n",
      "3       1664.0  231.997687  139.518217\n",
      "4       1792.0  232.160121  139.133016\n",
      "5       1920.0  232.138184  138.626575\n",
      "6       2048.0  231.296620  138.130695\n",
      "7       2176.0  231.214003  138.543144\n",
      "8       2304.0  231.371588  138.342684\n",
      "9       2432.0  231.363515  137.683523\n",
      "10      2560.0  231.344783  137.717751\n",
      "11      2688.0  231.423859  137.446061\n",
      "12      2816.0  232.385451  136.876816\n",
      "13      2944.0  231.448194  137.065260\n",
      "14      3072.0  231.397113  137.035920\n",
      "15      3200.0  230.693550  136.831230\n",
      "16      3328.0  231.515016  136.619544\n",
      "17      3456.0  230.728597  136.466531\n",
      "18      3584.0  230.765995  136.064471\n",
      "19      3712.0  230.685297  136.388811\n",
      "20      3840.0  230.736863  136.328603\n",
      "21      3968.0  230.777775  135.775980\n",
      "22      4096.0  230.741521  135.572368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size, seq_length, head_num, hidden_size = 4, 4096, 64, 128    # batch size, sequence length, head_dim, emb_dim\n",
    "rotary_percent = 0.5\n",
    "mode = 'forward'\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seq_length'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(10, 33)],  # different possible values for `x_name`\n",
    "        #x_vals=[2048, 4096],\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')], #('orage', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"RoPE performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={\"batch_size\": batch_size, \"head_num\": head_num, \"hidden_size\": hidden_size, \n",
    "              \"rotary_percent\": rotary_percent, \"tensor_format\": 'sbhd', \"mode\": mode},\n",
    "    ))\n",
    "def benchmark(batch_size, seq_length, head_num, hidden_size, provider, mode='forward', rotary_percent=1.0, tensor_format='sbhd'):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    t = torch.rand((batch_size, seq_length, head_num, hidden_size), dtype=torch.float32, device=device)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "        tensor_format == \"sbhd\"\n",
    "#     if transpose:\n",
    "#         t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    if provider == 'triton':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "#     if provider == 'cuda':\n",
    "#         def fwd(t, emb, tensor_format):\n",
    "#             return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=True)\n",
    "        \n",
    "    # forward pass\n",
    "    if mode == 'forward':       \n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fwd(t, emb, tensor_format), quantiles=quantiles)\n",
    "    if mode == 'backward':\n",
    "        y = fwd(t, emb, tensor_format)\n",
    "        dy = torch.randn_like(y)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True), quantiles=quantiles, grad_to_none=[t])\n",
    "\n",
    "    gbps = lambda ms: 2*t.numel() * t.element_size() / ms * 1e-6\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True, save_path='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
