{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6120af",
   "metadata": {},
   "source": [
    "# **Rotary Potion Embedding (RoPE) with Triton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99cc7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention.\"\"\"\n",
    "import collections\n",
    "from contextlib import nullcontext\n",
    "import functools\n",
    "# from importlib.metadata import version\n",
    "import math\n",
    "import os\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "# from pkg_resources import packaging\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc6e68",
   "metadata": {},
   "source": [
    "### **RoPE in Pytoch from TransformerEngine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5551e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Rotary Position Embedding from https://arxiv.org/abs/2104.09864.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        rotary_percent: float = 1.0,\n",
    "        seq_len_interpolation_factor: Optional[int] = None,\n",
    "        pretrained_max_position_embeddings: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dim: int\n",
    "            rotary embedding dimension\n",
    "        rotary_percent: float\n",
    "            Percent of rotary dimension to use for rotary position embeddings.\n",
    "        seq_len_interpolation_factor: int\n",
    "            if not None, discrete positions will be interpolated by this factor via the trick in\n",
    "            https://arxiv.org/abs/2306.15595\n",
    "        pretrained_max_position_embeddings: int\n",
    "            pre-trained max_position_embeddings before position interpolation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rotary_percent < 1.0:\n",
    "            dim = int(dim * rotary_percent)\n",
    "        self.seq_len_interpolation_factor = seq_len_interpolation_factor\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (\n",
    "                torch.arange(0, dim, 2, dtype=torch.float32, device=torch.cuda.current_device())\n",
    "                / dim\n",
    "            )\n",
    "        )\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.pretrained_max_position_embeddings = pretrained_max_position_embeddings\n",
    "\n",
    "    def forward(self, max_seq_len: int, offset: int = 0):\n",
    "        \"\"\"\n",
    "        Create rotary position embedding frequencies\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_seq_len: int\n",
    "            sequence length of a sample\n",
    "        offset: int, default = 0\n",
    "            fixed offset for freqencies\n",
    "        \"\"\"\n",
    "        seq = (\n",
    "            torch.arange(max_seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "            + offset\n",
    "        )\n",
    "\n",
    "        if (self.pretrained_max_position_embeddings is not None\n",
    "            and self.seq_len_interpolation_factor is not None):\n",
    "            if (max_seq_len >\n",
    "                self.pretrained_max_position_embeddings * self.seq_len_interpolation_factor):\n",
    "                # dynamic linear scaling (length > position we have learned)\n",
    "                seq *= 1 / (max_seq_len / self.pretrained_max_position_embeddings)\n",
    "            else:\n",
    "                # fixed linear scaling\n",
    "                seq *= 1 / self.seq_len_interpolation_factor\n",
    "\n",
    "        freqs = torch.einsum('i , j -> i j', seq, self.inv_freq)\n",
    "        # first part even vector components, second part odd vector components,\n",
    "        #  2 * dim in dimension size\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        # emb [seq_length, .., dim]\n",
    "        return emb.reshape(emb.size(0), 1, 1, emb.size(1))\n",
    "\n",
    "def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    change sign so the last dimension becomes [-odd, +even]\n",
    "    \"\"\"\n",
    "    x = x.view(x.shape[:-1] + torch.Size((2, x.shape[-1] // 2)))\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(\n",
    "    t: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    "    tensor_format: str = \"sbhd\",\n",
    "    fused: bool = False,\n",
    "    cu_seqlens: Union[torch.Tensor, None] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply rotary positional embedding tensor to the input tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: torch.Tensor\n",
    "        Input tensor of shape `[s, b, h, d]`, `[s, b, h, d]` or `[t, h, d]`, on which\n",
    "        rotary positional embedding will be applied.\n",
    "    freqs: torch.Tensor\n",
    "        Rotary positional embedding tensor of shape `[s2, 1, 1, d2]` and dtype 'float',\n",
    "        with `s2 >= s` and `d2 <= d`.\n",
    "    fused: bool, default = False\n",
    "        Whether to use a fused applying RoPE implementation.\n",
    "    tensor_format: {'sbhd', 'bshd', 'thd'}, default = 'sbhd'\n",
    "        is `bshd` if `t` is of shape `[bs, seq, ...]`, or `sbhd` if `t` is\n",
    "        of shape `[seq, bs, ...]`. 'thd' is only supported when `fused` is True.\n",
    "    cu_seqlens: torch.Tensor, default = None.\n",
    "        Cumulative sum of sequence lengths in a batch for `t`, with shape [b + 1] and\n",
    "        dtype torch.int32. Only valid when `tensor_format` is 'thd'.\n",
    "    \"\"\"\n",
    "#     if fused:\n",
    "#         assert (\n",
    "#             tensor_format != \"thd\" or cu_seqlens is not None\n",
    "#         ), \"cu_seqlens must not be None when tensor_format is 'thd'.\"\n",
    "#         return FusedRoPEFunc.apply(t, freqs, tensor_format, cu_seqlens)\n",
    "\n",
    "    assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "        \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "        f\"when fused is False, got {tensor_format}.\"\n",
    "    )\n",
    "\n",
    "    max_seq_len = freqs.shape[0]\n",
    "    cur_seq_len = t.shape[1] if tensor_format == \"bshd\" else t.shape[0]\n",
    "\n",
    "    # Only apply the rotary embeddings up to the sequence length of the running\n",
    "    # input.\n",
    "    assert cur_seq_len <= max_seq_len, (\n",
    "        f\"Rotary Embeddings only supported up to {max_seq_len} sequence length!\"\n",
    "    )\n",
    "    freqs = freqs[:cur_seq_len]\n",
    "    if tensor_format == \"bshd\":\n",
    "        freqs = freqs.transpose(0, 1)  # [seq, 1, 1, dim] -> [1, seq, 1, dim]\n",
    "    # cos/sin first then dtype conversion for better precision\n",
    "    cos_ = torch.cos(freqs).to(t.dtype)\n",
    "    sin_ = torch.sin(freqs).to(t.dtype)\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    # ideally t_pass is empty so rotary pos embedding is applied to all tensor t\n",
    "    t, t_pass = t[..., :rot_dim], t[..., rot_dim:]\n",
    "\n",
    "    # first part is cosine component\n",
    "    # second part is sine component, need to change signs with _rotate_half method\n",
    "    t = (t * cos_) + (_rotate_half(t) * sin_)\n",
    "    return torch.cat((t, t_pass), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed767511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tol(dtype: torch.dtype) -> Dict:\n",
    "    if dtype == torch.bfloat16:\n",
    "        return dict(atol=1e-2, rtol=1e-2)\n",
    "    elif dtype == torch.float16:\n",
    "        return dict(atol=1e-3, rtol=1e-3)\n",
    "    return dict(atol=1e-5, rtol=1.3e-6)\n",
    "\n",
    "\n",
    "# Gradient is a broadcasted scalar\n",
    "def _overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    return output.sum() * 2\n",
    "\n",
    "# Gradient is a full tensor\n",
    "def _non_overlapping_grad(output: torch.Tensor) -> torch.Tensor:\n",
    "    t = torch.ones_like(output)\n",
    "    return torch.sum(output * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a05d8",
   "metadata": {},
   "source": [
    "## **Triton**\n",
    "### **RoPE Forward**\n",
    "* RoPE에서 배치 간, 시퀀스 간, head 간 연산은 독립적이다. 하나의 임베딩 안에서 위치 변화(_rotate_half)를 하기 때문에 임베딩 내 연산만 메모리 접근 측면에서 독립적이지 않다.\n",
    "따라서 BLOCK_SIZE를 RoPE 연산을 적용하는 임베딩 차원의 크기로 설정한다. rotate_percent에 따라 RoPE 연산을 적용하는 임베딩의 갯수가 다르기 때문에 입력 텐서 $t$의 차원이 아닌 frequency embedding의 차원 수로 설정한다. 이 경우 한 블록 내에서 [1,1,1, freq_embedding_dim]의 메모리 접근이 가능하고 스레드마다 하나의 output을 계산한다.\n",
    "* 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & -\\sin m\\theta\\\\\n",
    "   \\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}\n",
    "$로 임베딩을 회전 변환하면 $(x_1\\cos m\\theta - x_2\\sin m\\theta, x_2\\cos m\\theta + x_1\\sin m\\theta)$의 결과가 나온다.\n",
    "여기서 sin, cos은 freq_emb(emb_ptr)을 순서대로 load하여 계산한다.\n",
    "_rotate_half를 하기 위해 입력 텐서(t_ptr)을 각각 t0, t1에 절반씩 load한다.\n",
    "out_ptr에 저장할 때 앞의 절반과 뒤의 절반의 연산이 다르기 때문에 각가 계산하여 다음과 같이 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291e0e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def rope_fwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "    \n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos - t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos + t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9d8ee",
   "metadata": {},
   "source": [
    "### **RoPE Backward**\n",
    "* RoPE forward와 마찬가지로 메모리 접근 측면에서 배치, 시퀀스, head 간 연산은 독립적이고 임베딩 간의 연산은 독립적이지 않기 때문에 BLOCK_SIZE를 frequency embedding의 차원 수로 설정한다.\n",
    "* backpropagation은 행렬 $\\begin{equation}\n",
    "   \\begin{pmatrix} \n",
    "   \\cos m\\theta & \\sin m\\theta\\\\\n",
    "   -\\sin m\\theta & \\cos m\\theta \\\\\n",
    "   \\end{pmatrix} \n",
    "\\end{equation}$로 회전변환하면 된다. 따라서 rope_fwd()에서 결과값을 저장하는 부분에서 sin 앞의 부호만 바꿔주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb132bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def rope_bwd(t_ptr, emb_ptr, out_ptr, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE:tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    t_start = pid * hidden_size\n",
    "    emb_start = pid//(batch_size*head_num) * BLOCK_SIZE\n",
    "\n",
    "    emb_off = emb_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    emb = tl.load(emb_ptr + emb_off, mask = emb_off < seq_length*BLOCK_SIZE)\n",
    "    _cos, _sin = tl.cos(emb), tl.sin(emb)\n",
    "\n",
    "    t_off0 = t_start + tl.arange(0, BLOCK_SIZE//2)\n",
    "    t_off1 = t_start + tl.arange(0, BLOCK_SIZE//2) + BLOCK_SIZE//2\n",
    "    mask_t0 = t_off0 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    mask_t1 = t_off1 < (seq_length * batch_size * head_num * hidden_size)\n",
    "    \n",
    "    t0 = tl.load(t_ptr + t_off0, mask = mask_t0)\n",
    "    t1 = tl.load(t_ptr + t_off1, mask = mask_t1)\n",
    "    \n",
    "    tl.store(out_ptr + t_off0, t0*_cos + t1*_sin, mask = mask_t0)\n",
    "    tl.store(out_ptr + t_off1, t1*_cos - t0*_sin, mask = mask_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c4151",
   "metadata": {},
   "source": [
    "### **RoPE Function**\n",
    "* rotary_percent에 따라 rotation을 하는 텐서 비율이 달라진다. rotation을 안하는 경우는 기존의 input 텐서 값을 그대로 보존하기 때문에 rope_fwd, rope_bwd 커널에 넣기 전에 output 텐서 값을 input 텐서 값으로 초기화한다. (복사하지 않아도 되는 값도 불러오기 때문에 시간이 더 소모될 수 있다고 예상한다.)\n",
    "* 가속화 하기 위해서는 BLOCK_SIZE를 1024에 가깝게 하는 게 좋다. 따라서 head_num도 같이 flatten하여 넣어주면 빨라질 것으로 예상하지만 시간 관계상 패스했다.\n",
    "* forward에서 받은 t와 freqs 텐서를 backward에서 다시 사용하기 위해 ctx.save_for_backward()로 저장한다.\n",
    "* BLOCK_SIZE는 freqs 텐서의 마지막 차원과 같은 크기로 지정한다.\n",
    "* grid는 input 텐서의 모든 차원 값을 곱한 값에서 BLOCK_SIZE로 나눈 값과 같다.\n",
    "* rope_fwd에서 출력된 텐서의 원소들의 메모리 주소가 연속적이지 않을 수 있다. 이를 해결하기 위해 contiguous()를 사용하여 정렬한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e73b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tritonRoPE(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, t, freqs, tensor_format: str = \"sbhd\",\n",
    "                cu_seqlens: Union[torch.Tensor, None] = None,):\n",
    "        \n",
    "        assert tensor_format in (\"sbhd\", \"bshd\"), (\n",
    "            \"Only formats `sbhd` or `bshd` are supported for input tensor `t` \"\n",
    "            f\"when fused is False, got {tensor_format}.\"\n",
    "        )\n",
    "        # allocate output\n",
    "#         if tensor_format == \"sbhd\":\n",
    "#             output = t.clone().detach()\n",
    "#             seq_length = t.shape[0]\n",
    "#             batch_size = t.shape[1]\n",
    "#             head_num = t.shape[2]\n",
    "#             hidden_size = t.shape[3]\n",
    "#             rot_dim = freqs.size()[-1]\n",
    "#             BLOCK_SIZE=rot_dim\n",
    "#             grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "#             rope_fwd[grid](t, freqs, output, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "        if tensor_format == \"bshd\":\n",
    "            t = t.transpose(0, 1).contiguous()\n",
    "            freqs = freqs.transpose(0, 1).contiguous()\n",
    "            \n",
    "        output = t.clone().detach()\n",
    "        seq_length = t.shape[0]\n",
    "        batch_size = t.shape[1]\n",
    "        head_num = t.shape[2]\n",
    "        hidden_size = t.shape[3]\n",
    "        rot_dim = freqs.size()[-1]\n",
    "        BLOCK_SIZE=rot_dim\n",
    "        grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "        rope_fwd[grid](t, freqs, output, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "        \n",
    "            \n",
    "        ctx.save_for_backward(freqs, cu_seqlens)\n",
    "        ctx.tensor_format = tensor_format\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor):\n",
    "        freqs, cu_seqlens = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone().detach()\n",
    "        if ctx.tensor_format == \"sbhd\":\n",
    "            seq_length = grad_output.shape[0]\n",
    "            batch_size = grad_output.shape[1]\n",
    "            head_num = grad_output.shape[2]\n",
    "            hidden_size = grad_output.shape[3]\n",
    "            rot_dim = freqs.size()[-1]\n",
    "            grad_output = grad_output.contiguous()\n",
    "            grid = lambda meta: (triton.cdiv(seq_length*batch_size*head_num*hidden_size, meta['BLOCK_SIZE']),)\n",
    "            rope_bwd[grid](grad_output, freqs, grad_input, seq_length, batch_size, head_num, hidden_size, BLOCK_SIZE=rot_dim)\n",
    "\n",
    "        return grad_input, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e9107",
   "metadata": {},
   "source": [
    "### **Test RoPE**\n",
    "TransformerEngine에 있는 RoPE와 Triton kernel의 RoPE의 결과가 동일한지 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefc12f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## PASSED ##########\n"
     ]
    }
   ],
   "source": [
    "# @pytest.mark.parametrize(\"dtype\", [torch.float32, torch.bfloat16, torch.float16])\n",
    "# @pytest.mark.parametrize(\"seq_length\", [2048, 4096])\n",
    "# @pytest.mark.parametrize(\"hidden_size\", [128, 256])\n",
    "# @pytest.mark.parametrize(\"rotary_percent\", [0.5, 1.0])\n",
    "# @pytest.mark.parametrize(\"margin\", [0, 10])\n",
    "# @pytest.mark.parametrize(\"transpose\", [None, (0, 1), (2, 3)])\n",
    "# @pytest.mark.parametrize(\"tensor_format\", [\"sbhd\", \"bshd\"])\n",
    "# @pytest.mark.parametrize(\"loss_func\", [_overlapping_grad, _non_overlapping_grad])\n",
    "def test_fused_rope(\n",
    "    dtype: torch.dtype,\n",
    "    seq_length: int,\n",
    "    hidden_size: int,\n",
    "    rotary_percent: float,\n",
    "    margin: int,\n",
    "    transpose: Union[Tuple, None],\n",
    "    tensor_format: str,\n",
    "    loss_func: Callable,\n",
    ") -> None:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    batch_size, head_num = 2, 64\n",
    "    t = torch.rand(\n",
    "        (seq_length - margin, batch_size, head_num, hidden_size),\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "    if transpose:\n",
    "        t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "\n",
    "    # unfused\n",
    "    output_unfused = apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    loss_unfused = loss_func(output_unfused)\n",
    "    loss_unfused.backward()\n",
    "    grad_unfused = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "    \n",
    "    #print(t.shape, emb.shape, output_unfused.shape)\n",
    "    \n",
    "    # Triton\n",
    "    output_triton = tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "    loss_triton = loss_func(output_triton)\n",
    "    loss_triton.backward()\n",
    "    grad_triton = t.grad.detach().clone()\n",
    "    t.grad = None\n",
    "\n",
    "    torch.testing.assert_close(output_triton, output_unfused)\n",
    "    torch.testing.assert_close(grad_triton, grad_unfused)\n",
    "\n",
    "#     torch.testing.assert_close(output_fused, output_unfused, **get_tol(dtype))\n",
    "#     torch.testing.assert_close(grad_fused, grad_unfused, **get_tol(dtype))\n",
    "#     assert output_fused.is_contiguous()\n",
    "    \n",
    "test_fused_rope(torch.float32, 2048, 128, 0.5, 10, None, \"sbhd\", _overlapping_grad)\n",
    "test_fused_rope(torch.float32, 2048, 128, 1.0, 0, None, \"sbhd\", _overlapping_grad)\n",
    "print(\"########## PASSED ##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3536b60c",
   "metadata": {},
   "source": [
    "### **Compare the Performance**\n",
    "시퀀스 길이(sequence_length)에 따른 Pytorch, Triton kernel, CUDA kernel로 작성한 RoPE의 성능을 비교했다. 배치 사이즈는 4, head 개수는 64, hidden_size는 128로 고정하고, sequence_length를 1280부터 4096까지 128씩 증가하여 성능을 확인했다. 모든 sequence length에 대해 Triton kernel이 기본 Pytorch 코드보다 forward와 backward 모두 2배 정도 빠른 것을 확인할 수 있다.  \n",
    "sequence_length를 x_vals로 주었지만, 배치 사이즈(batch_size), head의 개수(head_num), 임베딩 차원(hidden_size) 등으로 설정하여 이에 따른 성능도 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_length, head_num, hidden_size = 4, 4096, 64, 128    # batch size, sequence length, head_dim, emb_dim\n",
    "rotary_percent = 0.5\n",
    "mode = 'forward'\n",
    "\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['seq_length'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(10, 33)],  # different possible values for `x_name`\n",
    "        #x_vals=[2048, 4096],\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg``\n",
    "        line_names=[\"Triton\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', '-'), ('green', '-')], #('orage', '-')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"RoPE performance\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={\"batch_size\": batch_size, \"head_num\": head_num, \"hidden_size\": hidden_size, \n",
    "              \"rotary_percent\": rotary_percent, \"tensor_format\": 'sbhd', \"mode\": mode},\n",
    "    ))\n",
    "def benchmark(batch_size, seq_length, head_num, hidden_size, provider, mode='forward', rotary_percent=1.0, tensor_format='sbhd'):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    t = torch.rand((batch_size, seq_length, head_num, hidden_size), dtype=torch.float32, device=device)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if tensor_format == \"bshd\":\n",
    "        t = t.transpose(0, 1).contiguous()\n",
    "        tensor_format == \"sbhd\"\n",
    "#     if transpose:\n",
    "#         t = t.transpose(*transpose).contiguous().transpose(*transpose)\n",
    "    t.requires_grad = True\n",
    "\n",
    "    rotary_pos_emb = RotaryPositionEmbedding(hidden_size, rotary_percent)\n",
    "    emb = rotary_pos_emb(seq_length)\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=False)\n",
    "    if provider == 'triton':\n",
    "        def fwd(t, emb, tensor_format):\n",
    "            return tritonRoPE.apply(t, emb, tensor_format, None)\n",
    "#     if provider == 'cuda':\n",
    "#         def fwd(t, emb, tensor_format):\n",
    "#             return apply_rotary_pos_emb(t, emb, tensor_format=tensor_format, fused=True)\n",
    "        \n",
    "    # forward pass\n",
    "    if mode == 'forward':       \n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fwd(t, emb, tensor_format), quantiles=quantiles)\n",
    "    if mode == 'backward':\n",
    "        y = fwd(t, emb, tensor_format)\n",
    "        dy = torch.randn_like(y)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True), quantiles=quantiles, grad_to_none=[t])\n",
    "\n",
    "    gbps = lambda ms: 2*t.numel() * t.element_size() / ms * 1e-6\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True, save_path='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
